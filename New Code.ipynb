{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d5a67d5-19de-4f46-ac7c-bdf87b9253f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ompl import util as ou\n",
    "from ompl import base as ob\n",
    "from ompl import geometric as og\n",
    "from shapely import Point, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc940f30-ddbc-4acb-bccd-3fd7c3bd02e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads data from a JSON file and converts it into a list of Python objects.\n",
    "    \n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: List of Python objects.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    with open(file_path, \"r\") as json_file:\n",
    "        for line in json_file:\n",
    "            data_list.append(json.loads(line))\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cfe2a6-cfd5-4f12-a4a0-38ecfe040395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_occurrences_of_first_element(coordinates):\n",
    "    #function that counts the duplicates of our first number\n",
    "    #we use this function to help us remove the starting coordinates that repeat when our human player\n",
    "    #has yet to move\n",
    "    first_element = coordinates[0]\n",
    "    count = coordinates.count(first_element)\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2582b223-b872-44fd-8d78-17343e99686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_paths_raw = load_json_data(\"data/human_paths.json\")\n",
    "#this takes in the raw coordinates of our human paths\n",
    "first_to_cross_data = load_json_data(\"data/first_to_cross.json\")\n",
    "#stores who crossed our doorway first in our human paths, we use this to determine the effectivness of our model later on \n",
    "human_goals_data = load_json_data(\"data/human_goals.json\")\n",
    "#registers the goals for each human agent, we set this as our goal for the robots too\n",
    "total_rounds = len(first_to_cross_data)\n",
    "#number of overall rounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8dce507-aa61-451e-a67e-2d78f70f85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO READ THE DATA,\n",
    "\n",
    "#human_paths_raw[round][player][time/step][0 for x or 1 for y]\n",
    "\n",
    "#To access the goals\n",
    "\n",
    "#human_goals_data[round][player][0][0 for x or 1 for y]\n",
    "\n",
    "#To access the order of crossing\n",
    "\n",
    "# first_to_cross_data[round]\n",
    "# This gives 1 if player one (left side) crossed first, or 2 if they crossed second.\n",
    "\n",
    "# This information is crucial for evaluating our model, as we aim for the model to\n",
    "# prefer the strategy that matches the observed order of crossing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d3fb6b-bf9e-4eb3-bb6b-edc1d84ec7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5570b4-2d03-45f1-b628-865690d0c3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1470e097-5b49-405b-8c08-55b86cb371b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we create an array to store the smaller number of duplicates between two players in the same round\n",
    "min_duplicates_human_paths = []\n",
    "#we plan to remove the smaller number of duplicates, as this will disregard time where the players have yet to start but we still are saving their cooridnates \n",
    "\n",
    "for i in range(total_rounds):\n",
    "    min_duplicates_human_paths.append(min(count_occurrences_of_first_element(human_paths_raw[i][0]),  count_occurrences_of_first_element(human_paths_raw[i][1])))\n",
    "\n",
    "#now in min_min_duplicates_human_paths, the i'th element has the smaller number of duplicates between two players in round i\n",
    "#we use this to remove the duplicates at the beginning, but only the min number as we want our paths to still have the same number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dad559-1879-4a32-a96b-1807dafa2be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20978e3f-9b20-4674-a005-540c71c87e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_paths_improved = []\n",
    "#this array will have our human paths but remove the first duplicates\n",
    "for round in range(total_rounds):\n",
    "    temp_path = []\n",
    "    for p in range(2):\n",
    "        temp_path.append(human_paths_raw[round][p][min_duplicates_human_paths[round]: ])\n",
    "    human_paths_improved.append(temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980294ed-e629-46aa-a108-23b0d5d5ff15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e4be799-d3f3-4873-80db-b3b3108f7fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define our obstacles\n",
    "obstacles = []\n",
    "\n",
    "obstacle_1 = Polygon([(-3, 10),(3, 10), (3, 2),(-3, 2)])\n",
    "obstacles.append(obstacle_1)\n",
    "\n",
    "obstacle_2 = Polygon([(-3, -2), (3, -2),(3, -10), (-3, -10)])\n",
    "obstacles.append(obstacle_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "193491e3-0729-4be6-be1d-c2e4e14fbf45",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# We store the strategies generated by RRT that match in order of crossing in the following\n",
    "matching_strategies = []\n",
    "# We store the strategies generated by RRT that don't match in order of crossing in the following\n",
    "non_matching_strategies = []\n",
    "\n",
    "# Create two state validity checkers. \n",
    "# One forces the strategy generated to have player one cross first, (isStateValid_player1_first)\n",
    "# and the other forces player two to cross first. (isStateValid_player2_first)\n",
    "\n",
    "def isStateValid_player1_first(state):\n",
    "    # Extract agent coordinates from the state.\n",
    "    x1 = state[0]\n",
    "    y1 = state[1]\n",
    "    x2 = state[2]\n",
    "    y2 = state[3]\n",
    "\n",
    "    agent1 = Point(x1, y1).buffer(1)\n",
    "    agent2 = Point(x2, y2).buffer(1)\n",
    "\n",
    "    # forces the strategy generated to have the first agent to cross first.\n",
    "    if x2 < 3 and x1 < 3:\n",
    "        return False\n",
    "\n",
    "    # Check if either agent is outside the bounds of the environment.\n",
    "    if x1 > 10 or x1 < -10 or y1 > 10 or y1 < -10 or x2 > 10 or x2 < -10 or y2 > 10 or y2 < -10:\n",
    "        return False\n",
    "\n",
    "    # Check if either agent intersects with any obstacles or with each other.\n",
    "    for obstacle in obstacles:\n",
    "        if obstacle.intersects(agent1) or obstacle.intersects(agent2) or agent1.intersects(agent2):\n",
    "            return False\n",
    "            \n",
    "    # Return True if all checks pass, False otherwise.\n",
    "    return True\n",
    "\n",
    "def isStateValid_player2_first(state):\n",
    "    # Extract agent coordinates from the state.\n",
    "    x1 = state[0]\n",
    "    y1 = state[1]\n",
    "    x2 = state[2]\n",
    "    y2 = state[3]\n",
    "\n",
    "    agent1 = Point(x1, y1).buffer(1)\n",
    "    agent2 = Point(x2, y2).buffer(1)\n",
    "\n",
    "    # forces the strategy generated to have the second agent to cross first.\n",
    "    if x1 > -3 and x2 > -3:\n",
    "        return False\n",
    "\n",
    "    # Check if either agent is outside the bounds of the environment.\n",
    "    if x1 > 10 or x1 < -10 or y1 > 10 or y1 < -10 or x2 > 10 or x2 < -10 or y2 > 10 or y2 < -10:\n",
    "        return False\n",
    "\n",
    "    # Check if either agent intersects with any obstacles or with each other.\n",
    "    for obstacle in obstacles:\n",
    "        if obstacle.intersects(agent1) or obstacle.intersects(agent2) or agent1.intersects(agent2):\n",
    "            return False\n",
    "            \n",
    "    # Return True if all checks pass, False otherwise.\n",
    "    return True\n",
    "\n",
    "\n",
    "#generates the strategies (paths for both players) that force player one to cross first\n",
    "def plan_player1_first(i):\n",
    "    # Create a state space, set its bounds, and create a simple setup.\n",
    "    space = ob.RealVectorStateSpace(4)\n",
    "    bounds = ob.RealVectorBounds(4)\n",
    "    bounds.setLow(-10)\n",
    "    bounds.setHigh(10)\n",
    "    space.setBounds(bounds)\n",
    "\n",
    "    # Create a simple setup with the 4D state space\n",
    "    ss = og.SimpleSetup(space)\n",
    "\n",
    "    # Set the state validity checker to isStateValid_player1_first.\n",
    "    ss.setStateValidityChecker(ob.StateValidityCheckerFn(isStateValid_player1_first))\n",
    "\n",
    "    # Create start and goal states based on the human paths and goals.\n",
    "    start = ob.State(space)\n",
    "    start[0] = human_paths_improved[i][0][0][0]\n",
    "    start[1] = human_paths_improved[i][0][0][1]\n",
    "    start[2] = human_paths_improved[i][1][0][0]\n",
    "    start[3] = human_paths_improved[i][1][0][1]\n",
    "\n",
    "    goal = ob.State(space)\n",
    "    goal[0] = human_goals_data[i][0][0][0]\n",
    "    goal[1] = human_goals_data[i][0][0][1]\n",
    "    goal[2] = human_goals_data[i][1][0][0]\n",
    "    goal[3] = human_goals_data[i][1][0][1]\n",
    "\n",
    "    # Set the start and goal states for the simple setup.\n",
    "    ss.setStartAndGoalStates(start, goal)\n",
    "\n",
    "    # Create an RRT planner and set it for the simple setup.\n",
    "    planner = og.RRTstar(ss.getSpaceInformation())\n",
    "    ss.setPlanner(planner)\n",
    "\n",
    "    # Try to solve the problem within a time limit.\n",
    "    solved = ss.solve(5.0)\n",
    "    if solved:\n",
    "        # Interpolate the path to match the length of the human path.\n",
    "        path = ss.getSolutionPath()\n",
    "        path.interpolate(len(human_paths_improved[i][0])) \n",
    "\n",
    "        # Extract agent coordinates from the path.\n",
    "        agent1data = []\n",
    "        agent2data = []\n",
    "        for state in path.getStates():\n",
    "            x1 = state[0]\n",
    "            y1 = state[1]\n",
    "            x2 = state[2]\n",
    "            y2 = state[3]\n",
    "            agent1data.append((x1, y1))\n",
    "            agent2data.append((x2, y2))\n",
    "        temp_path = [agent1data, agent2data]\n",
    "\n",
    "        # If the actual first player to cross matches with the strategy generated, \n",
    "        # add the strategy to the matching strategies, else to the non-matching strategies.\n",
    "        if first_to_cross_data[i][0] == 1:\n",
    "            matching_strategies.append(temp_path)\n",
    "        else:\n",
    "            non_matching_strategies.append(temp_path)\n",
    "    else:\n",
    "        print(\"no solution\")\n",
    "\n",
    "#generates the strategies (paths for both players) that force player two to cross first\n",
    "def plan_player2_first(i):\n",
    "    # Same steps as plan_player1_first, but with a different state validity checker.\n",
    "    space = ob.RealVectorStateSpace(4)\n",
    "    bounds = ob.RealVectorBounds(4)\n",
    "    bounds.setLow(-10)\n",
    "    bounds.setHigh(10)\n",
    "    space.setBounds(bounds)\n",
    "\n",
    "    ss = og.SimpleSetup(space)\n",
    "\n",
    "    ss.setStateValidityChecker(ob.StateValidityCheckerFn(isStateValid_player2_first))\n",
    "\n",
    "    start = ob.State(space)\n",
    "    start[0] = human_paths_improved[i][0][0][0]\n",
    "    start[1] = human_paths_improved[i][0][0][1]\n",
    "    start[2] = human_paths_improved[i][1][0][0]\n",
    "    start[3] = human_paths_improved[i][1][0][1]\n",
    "\n",
    "    goal = ob.State(space)\n",
    "    goal[0] = human_goals_data[i][0][0][0]\n",
    "    goal[1] = human_goals_data[i][0][0][1]\n",
    "    goal[2] = human_goals_data[i][1][0][0]\n",
    "    goal[3] = human_goals_data[i][1][0][1]\n",
    "\n",
    "    ss.setStartAndGoalStates(start, goal)\n",
    "\n",
    "    planner = og.RRTstar(ss.getSpaceInformation())\n",
    "    ss.setPlanner(planner)\n",
    "\n",
    "    solved = ss.solve(5.0)\n",
    "    if solved:\n",
    "        path = ss.getSolutionPath()\n",
    "        path.interpolate(len(human_paths_improved[i][0]))\n",
    "\n",
    "        agent1data = []\n",
    "        agent2data = []\n",
    "        for state in path.getStates():\n",
    "            x1 = state[0]\n",
    "            y1 = state[1]\n",
    "            x2 = state[2]\n",
    "            y2 = state[3]\n",
    "            agent1data.append((x1, y1))\n",
    "            agent2data.append((x2, y2))\n",
    "        temp_path = [agent1data, agent2data]\n",
    "        if first_to_cross_data[i][0] == 2:\n",
    "            matching_strategies.append(temp_path)\n",
    "        else:\n",
    "            non_matching_strategies.append(temp_path)\n",
    "    else:\n",
    "        print(\"no solution\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee1693f-e813-4871-b5e9-f97300afdc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f93530c-4a38-46b7-94cf-a7860e0c1587",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 31.88 in 655 iterations (113 vertices in the graph)\n",
      "Info:    RRTstar: Created 638 new states. Checked 203841 rewire options. 1 goal states in tree. Final solution cost 26.178\n",
      "Info:    Solution found in 5.049802 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 41.76 in 678 iterations (68 vertices in the graph)\n",
      "Info:    RRTstar: Created 565 new states. Checked 159895 rewire options. 1 goal states in tree. Final solution cost 27.855\n",
      "Info:    Solution found in 5.040249 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 31.86 in 694 iterations (111 vertices in the graph)\n",
      "Info:    RRTstar: Created 636 new states. Checked 202566 rewire options. 1 goal states in tree. Final solution cost 25.538\n",
      "Info:    Solution found in 5.048597 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 28.93 in 630 iterations (86 vertices in the graph)\n",
      "Info:    RRTstar: Created 603 new states. Checked 182106 rewire options. 1 goal states in tree. Final solution cost 23.867\n",
      "Info:    Solution found in 5.051060 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 41.49 in 142 iterations (26 vertices in the graph)\n",
      "Info:    RRTstar: Created 516 new states. Checked 133386 rewire options. 1 goal states in tree. Final solution cost 31.018\n",
      "Info:    Solution found in 5.051894 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 48.06 in 722 iterations (115 vertices in the graph)\n",
      "Info:    RRTstar: Created 598 new states. Checked 179101 rewire options. 1 goal states in tree. Final solution cost 31.486\n",
      "Info:    Solution found in 5.044024 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 27.08 in 244 iterations (36 vertices in the graph)\n",
      "Info:    RRTstar: Created 651 new states. Checked 212226 rewire options. 1 goal states in tree. Final solution cost 26.869\n",
      "Info:    Solution found in 5.036106 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 34.45 in 150 iterations (27 vertices in the graph)\n",
      "Info:    RRTstar: Created 621 new states. Checked 193131 rewire options. 1 goal states in tree. Final solution cost 24.465\n",
      "Info:    Solution found in 5.058979 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 48.92 in 685 iterations (75 vertices in the graph)\n",
      "Info:    RRTstar: Created 598 new states. Checked 179101 rewire options. 1 goal states in tree. Final solution cost 26.149\n",
      "Info:    Solution found in 5.032826 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 43.11 in 195 iterations (37 vertices in the graph)\n",
      "Info:    RRTstar: Created 625 new states. Checked 195625 rewire options. 1 goal states in tree. Final solution cost 24.273\n",
      "Info:    Solution found in 5.031338 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 38.42 in 543 iterations (78 vertices in the graph)\n",
      "Info:    RRTstar: Created 664 new states. Checked 220780 rewire options. 1 goal states in tree. Final solution cost 30.754\n",
      "Info:    Solution found in 5.041729 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 29.27 in 346 iterations (57 vertices in the graph)\n",
      "Info:    RRTstar: Created 681 new states. Checked 232221 rewire options. 1 goal states in tree. Final solution cost 25.152\n",
      "Info:    Solution found in 5.037968 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 38.43 in 465 iterations (72 vertices in the graph)\n",
      "Info:    RRTstar: Created 696 new states. Checked 242556 rewire options. 1 goal states in tree. Final solution cost 25.352\n",
      "Info:    Solution found in 5.041302 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 42.38 in 567 iterations (110 vertices in the graph)\n",
      "Info:    RRTstar: Created 665 new states. Checked 221445 rewire options. 1 goal states in tree. Final solution cost 31.962\n",
      "Info:    Solution found in 5.041584 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 44.01 in 525 iterations (77 vertices in the graph)\n",
      "Info:    RRTstar: Created 625 new states. Checked 195625 rewire options. 1 goal states in tree. Final solution cost 32.087\n",
      "Info:    Solution found in 5.035007 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 44.42 in 359 iterations (49 vertices in the graph)\n",
      "Info:    RRTstar: Created 667 new states. Checked 222778 rewire options. 1 goal states in tree. Final solution cost 27.105\n",
      "Info:    Solution found in 5.033663 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 46.98 in 378 iterations (41 vertices in the graph)\n",
      "Info:    RRTstar: Created 627 new states. Checked 196878 rewire options. 1 goal states in tree. Final solution cost 27.247\n",
      "Info:    Solution found in 5.035233 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 41.34 in 66 iterations (13 vertices in the graph)\n",
      "Info:    RRTstar: Created 695 new states. Checked 241860 rewire options. 1 goal states in tree. Final solution cost 23.747\n",
      "Info:    Solution found in 5.043777 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 54.01 in 517 iterations (74 vertices in the graph)\n",
      "Info:    RRTstar: Created 616 new states. Checked 190036 rewire options. 1 goal states in tree. Final solution cost 26.718\n",
      "Info:    Solution found in 5.057035 seconds\n",
      "Debug:   RRTstar: Planner range detected to be 8.000000\n",
      "Info:    RRTstar: No optimization objective specified. Defaulting to optimizing path length for the allowed planning time.\n",
      "Info:    RRTstar: Started planning with 1 states. Seeking a solution better than 0.00000.\n",
      "Info:    RRTstar: Initial k-nearest value of 83\n",
      "Info:    RRTstar: Found an initial solution with a cost of 26.46 in 192 iterations (31 vertices in the graph)\n",
      "Info:    RRTstar: Created 673 new states. Checked 226801 rewire options. 1 goal states in tree. Final solution cost 22.402\n",
      "Info:    Solution found in 5.094975 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(total_rounds):\n",
    "    plan_player1_first(i)\n",
    "    plan_player2_first(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc16bf1-e4a2-47d9-97a1-1416a50d6b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba908d9-b1e7-4845-b784-4aea04799180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(human_path, generated_path, n):\n",
    "    #error function to compare two paths, this will be used in updating our agent's opinions\n",
    "    #it calculates the difference between paths the next n steps\n",
    "    e = 0\n",
    "    for i in range(n):\n",
    "        e += math.dist(human_path[i], generated_path[i])\n",
    "    return e/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82697019-6374-4f61-8943-161ee7e5682f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de574f2b-abdd-4fd7-83ed-7416d6de833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_model(matching_strategy, non_matching_strategy, human_strategy, u, n):\n",
    "\n",
    "    # n is how frequent our agent updates its opinon, every n steps\n",
    "    #u is the weight of the social incentives of other agents, currently in this code it won't make a difference as we have no ratioanl incentives \n",
    "    \n",
    "    \n",
    "    #this will be our agents opinion on the two paths \n",
    "    z = [0,0] \n",
    "    \n",
    "    length_of_paths = len(matching_strategy) #this will be the same across all 3 paths\n",
    "    \n",
    "    for j in range(length_of_paths // n -1): \n",
    "        #this for loop alllows us to iteratively update our opinion every n steps\n",
    "\n",
    "        error_strategy_0 = -error(matching_strategy[j*n:], human_strategy[j*n:], n)\n",
    "        error_strategy_1 = -error(non_matching_strategy[j*n:], human_strategy[j*n:], n)\n",
    "        #these two will be the error between each teh strategies each n steps\n",
    "\n",
    "        relative_error_strategy_0 = math.tanh(error_strategy_0 - error_strategy_1) + 1 \n",
    "        relative_error_strategy_1 = math.tanh(error_strategy_1 - error_strategy_0) + 1\n",
    "        #these are the relative error estimates \n",
    "\n",
    "        z[0] = z[0] - (1/ (j + 1)) * (z[0] - u * relative_error_strategy_0)\n",
    "        z[1] = z[1] - (1/ (j + 1)) * (z[1] - u * relative_error_strategy_1)\n",
    "\n",
    "        print(z)\n",
    "    return z\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "188f1620-666f-4d6e-8b1a-e09ab84c7196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5229937777539484, 0.4770062222460515]\n",
      "[1.7411983454791782, 0.2588016545208218]\n",
      "[1.8263464778933793, 0.1736535221066206]\n",
      "[1.869705515189707, 0.13029448481029293]\n",
      "[1.8957636134693452, 0.10423638653065492]\n",
      "[1.9131362408253567, 0.08686375917464345]\n",
      "[1.9255453114591028, 0.07445468854089735]\n",
      "[1.9348519254316554, 0.0651480745683447]\n",
      "[1.9420885854027747, 0.057911414597225375]\n",
      "[1.9478402045795615, 0.05215979542043856]\n",
      "[1.9520303412688083, 0.0479696587311919]\n",
      "[1.9493208781601112, 0.05067912183988898]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9493208781601112, 0.05067912183988898]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_model(matching_strategies[0][0], non_matching_strategies[0][0], human_paths_improved[0][0], 1, 30)\n",
    "\n",
    "#this case we see agent 0 opinion on the matching strategy as time goes on \n",
    "#Round 0, Agent 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aac8c052-e5f1-43ba-8109-07687d1f53f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4868138670768278, 0.5131861329231721]\n",
      "[0.8012078157201789, 1.1987921842798213]\n",
      "[0.579925688333119, 1.420074311666881]\n",
      "[0.9044181892385461, 1.0955818107614541]\n",
      "[1.1235187675559017, 0.8764812324440986]\n",
      "[1.2695989589413645, 0.7304010410586357]\n",
      "[1.373941964460755, 0.6260580355392453]\n",
      "[1.4521992126951844, 0.5478007873048159]\n",
      "[1.5130655493628917, 0.4869344506371084]\n",
      "[1.5616957142791932, 0.4383042857208068]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5616957142791932, 0.4383042857208068]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_model(matching_strategies[0][1], non_matching_strategies[0][1], human_paths_improved[0][1], 1, 30)\n",
    "#agent 1's opinion on the same case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5325dd66-20e3-40c9-b727-fc9bdab0a260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
